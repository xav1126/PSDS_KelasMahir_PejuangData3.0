{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing (NLP) Dasar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9LXDwgaYwhj"
      },
      "source": [
        "#Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAMH4TkYouQ"
      },
      "source": [
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMU33UhZTJb"
      },
      "source": [
        "#Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhPKs6_IZAcC"
      },
      "source": [
        "Natural Language Processing (NLP) merupakan salah satu cabang ilmu AI yang berfokus pada pengolahan bahasa natural. Bahasa natural adalah bahasa yang secara umum digunakan oleh manusia dalam berkomunikasi satu sama lain. Bahasan yang diterima oleh komputer butuh untuk diproses dan dipahami terlebih dahulu supaya maksud dari user bisa dipahami dengan baik oleh komputer.\n",
        "\n",
        "Ada berbagai alasan terapan aplikasi dari NLP. Diantaranya adalah Chatbot (aplikasi yang membuat user bisa seolah-olah melakukan komunikasi dengan komputer). Stemming atau Lemmatization (Pemotongan kata dalam bahasa tertentu menjadi bentuk dasar pengenalan fungsi setiap kata dalam kalimat). Summarization (ringkasan komputer mampu memahami instruksi bahasa yang diinputkan oleh user.\n",
        "\n",
        "Pustejovsky dan Stubbs (2012) menjelaskan bahwa ada beberapa area utama penelitian pada field NLP diantaranya:\n",
        "\n",
        "1. **Question Answering System (QAS)**. Kemampuan komputer untuk menjawab pertanyaan yang diberikan oleh user. Daripada memasukkan keyword ke dalam browser pencarian, dengan QAS, user bisa langsung bertanya dalam bahasa natural yang digunakannya, baik itu Inggris, Mandarin, ataupun Indonesia.\n",
        "2. **Summarization**. Pembuatan ringkasan dari sekumpulan konten dokumen atau email. Dengan menggunakan aplikasi ini, user bisa dibantu untuk mengkonversikan dokumen teks yang besar ke dalam bentuk slide presentasi. Machine Translation produk yang dihasilkan adalah aplikasi yang dapat memahami bahasa manusia dan menterjemahkannya ke dalam bahasa lain, termasuk didalamnya adalah Google Translate yang apabila dicermati semakin membaik dalam penterjemahan bahasa. Contoh lain lagi adalah BabelFish yang menterjemahkan bahasa pada real time.\n",
        "3. **Speech Recognition**. Field ini merupakan cabang ilmu NLP yang cukup sulit. Proses pembangunan model untuk digunakan telpon/komputer dalam mengenali bahasa yang diucapkan sudah banyak dikerjakan. Bahasa yang sering digunakan adalah berupa pertanyaan atau perintah.\n",
        "4. **Document Classification**. Sedangkan aplikasi ini adalah area penelitian NLP yang paling sukses. Pekerjaan yang dilakukan aplikasi ini adalah menentukan dimana tempat terbaik dokumen yang baru diinputkan ke dalam sistem. Hal ini sangat berguna pada aplikasi spam filtering, news article classification, dan movie review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoXaGLHKZY-Q"
      },
      "source": [
        "#Scrapping Data Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40iYzuf-Zbvd"
      },
      "source": [
        "Sebelum melakukan penerapan dan berbagai penelitian, mengumpulkan data teks sebagai bahan dasar dari bidang ini merupakan hal yang sangat penting. Proses ini biasa disebut dengan scrapping data. Aktivitas scapping data bisa dilakukan melalui berbagai platfrom. Mulai langsung pada halaman web tertentu, melalui API seperti twitter, atau melalui tools yang sudah disediakan bisa free atau berbayar. Untuk mulai belajar NLP, kita akan menggunakan tools. Tools/Library google_play_scrapper adalah library yang dapat digunakan untuk mengambil review dari google apps. Pertama kita perlu melakukan instalasi sebagai berikut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prM_WEX4ZduE"
      },
      "source": [
        "#Instalasi google play scrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcO1w2F6Zj57",
        "outputId": "9262cf02-c8cc-4093-8b0e-d1d4980e6303"
      },
      "source": [
        "!pip install google_play_scraper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google_play_scraper\n",
            "  Downloading google-play-scraper-1.0.2.tar.gz (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 30 kB 44.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 40 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 51 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: google-play-scraper\n",
            "  Building wheel for google-play-scraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-play-scraper: filename=google_play_scraper-1.0.2-py3-none-any.whl size=24393 sha256=cb5082037929206c1356e5df6fc198571af3f0a90d39a8b3fd499204c85dab55\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/99/eb/bbb9d24a5c526980647efc10336eaaeffcf07749f581111128\n",
            "Successfully built google-play-scraper\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkPYGxLKZgWt"
      },
      "source": [
        "**Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEjN3yFFZrMG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google_play_scraper import Sort, reviews       # Library untuk scrapping data\n",
        "import re                                          # Library untuk teks"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM__kDfIZqW6"
      },
      "source": [
        "**Scrapping Data Review Teks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "SsJh0DTyaDEm",
        "outputId": "b41a0602-9a35-4dc3-db2d-37814ac0a215"
      },
      "source": [
        "Hasil_Scrapping = pd.read_csv('/content/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
        "Hasil_Scrapping.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Pasangan Calon</th>\n",
              "      <th>Text Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                         Text Tweet\n",
              "0   1  ...  Banyak akun kloning seolah2 pendukung #agussil...\n",
              "1   2  ...  #agussilvy bicara apa kasihan yaa...lap itu ai...\n",
              "2   3  ...  Kalau aku sih gak nunggu hasil akhir QC tp lag...\n",
              "3   4  ...  Kasian oh kasian dengan peluru 1milyar untuk t...\n",
              "4   5  ...  Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRu-BLFScIwV"
      },
      "source": [
        "**Mengambil Series Data Teks Review**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmvM_4HQcMd7",
        "outputId": "bffeac88-8f28-4803-8c00-477987aa1bdf"
      },
      "source": [
        "teks = Hasil_Scrapping['Text Tweet']\n",
        "teks"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      Banyak akun kloning seolah2 pendukung #agussil...\n",
              "1      #agussilvy bicara apa kasihan yaa...lap itu ai...\n",
              "2      Kalau aku sih gak nunggu hasil akhir QC tp lag...\n",
              "3      Kasian oh kasian dengan peluru 1milyar untuk t...\n",
              "4      Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...\n",
              "                             ...                        \n",
              "895    Kali saja bpk @aniesbaswedan @sandiuno lihat, ...\n",
              "896    Kita harus dapat merangkul semua orang tanpa b...\n",
              "897    Ini jagoanku dibidang digital <Smiling Face Wi...\n",
              "898                 #PesanBijak #OkeOce #GubernurGu3 ...\n",
              "899    Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...\n",
              "Name: Text Tweet, Length: 900, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQe_vRG-cQS_"
      },
      "source": [
        "#Teks Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-PhzDP9cS_v"
      },
      "source": [
        "Setelah mendapat data teks, salah satu tantangan dari data adalah bentuknya yang sangat beragam. Sebuah kata dapat ditulis dengan berbagai bentuk. Kemudian juga besar sekali kemungkinan adalah kesalahan penulisan, tanda baca, angka, dan lain-lain. Oleh sebab itu, sebelum diolah lebih lanjut untuk diproses menjadi data numerik, maka diperlukan pemrosesan data teks agar menjadi bentuk yang lebih bersih dan standar. Yang akan sangat mempengaruhi hasil analisis data teks tersebut. Pada sentimen analisis misalnya langkah ini menjadi sangat penting. Ada beberapa hal yang dilakukan pada tahap Teks Preprocessing:\n",
        "\n",
        "1. **Case Folding dan Data Cleaning**\n",
        "Case folding adalah salah satu bentuk text preprocessing yang paling sederhana dan efektif meskipun sering diabaikan. Tujuan dari case folding untuk mengubah semua huruf dalam dokumen menjadi huruf kecil. Hanya huruf 'a' sampai 'z' yang diterima. Karakter selain huruf dihilangkan dan dianggap delimiter.\n",
        "\n",
        "Ada beberapa cara yang dapat digunakan dalam tahap folding, diantaranya:\n",
        "\n",
        "Menghapus tanda baca\n",
        "Menghapus angka\n",
        "Mengubah text menjadi lowercase\n",
        "Menghapus whitepace (karakter kosong)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "Uqzoh58Nchue",
        "outputId": "fe4e73b6-c073-4e9b-b8dc-c691d7481caa"
      },
      "source": [
        "# Menghapus tanda baca\n",
        "print(teks[13])\n",
        "teks[13]=re.sub(r'[^\\w]|_',' ',teks[13])\n",
        "teks[13]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aku juga kemarin #AHY 19april2017 suaraku utk paslon 3 #DKI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aku juga kemarin  AHY 19april2017 suaraku utk paslon 3  DKI'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "C78m4I6xcnWa",
        "outputId": "e068d29a-dcb2-4621-9a08-957f18ac7f98"
      },
      "source": [
        "# Menghapus angka\n",
        "print(teks[200])\n",
        "teks[200]=re.sub(\"\\S*\\d\\S*\", \"\", teks[200]).strip()\n",
        "teks[200]=re.sub(r\"\\b\\d+\\b\", \" \", teks[200])\n",
        "teks[200]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngawal Abang dulu keliling Ampenan #AHY #fkppi #kawal #bodyguard #military #myjob https://www.instagram.com/p/BTySqVcjLv5/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ngawal Abang dulu keliling Ampenan #AHY #fkppi #kawal #bodyguard #military #myjob'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "shX0HNhNcsfp",
        "outputId": "97dd4f2b-f469-4db1-8f67-6d63efc88a88"
      },
      "source": [
        "# Mengubah text menjadi lowercase\n",
        "print(teks[397])\n",
        "\n",
        "teks[397]=teks[397].lower()\n",
        "teks[397]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#AhokDjarot membuktikan bahwa tak selamanya mantan itu digantikan dengan yang lebih baik #mantanterindah #akakakakakk #maapnyampah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#ahokdjarot membuktikan bahwa tak selamanya mantan itu digantikan dengan yang lebih baik #mantanterindah #akakakakakk #maapnyampah'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "9fpv6SOiczXW",
        "outputId": "a014ec1f-4688-4cc8-da13-600fcb7e5b5f"
      },
      "source": [
        "# Menghapus white space\n",
        "print(teks[28])\n",
        "\n",
        "teks[28]= re.sub('[\\s]+', ' ', teks[28])\n",
        "teks[28]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@AdjieRimbawan isu lama diangkat lagi, video lama dishare lagi, padahal video sudah disebar buzzer #AHY eh jelang #putaranke2 disebar lagi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@AdjieRimbawan isu lama diangkat lagi, video lama dishare lagi, padahal video sudah disebar buzzer #AHY eh jelang #putaranke2 disebar lagi'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WlooEbec35l"
      },
      "source": [
        "Membuat Fungsi untuk melakukan Case Folding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOeRm01yc3cL"
      },
      "source": [
        "import re, string, unicodedata\n",
        "def Case_Folding(text):\n",
        "    # Hapus non-ascii\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', '')\n",
        "    \n",
        "    # Menghapus Tanda baca\n",
        "    text = re.sub(r'[^\\w]|_',' ', text)\n",
        "    \n",
        "    # Menghapus angka\n",
        "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
        "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
        "    \n",
        "    # Mengubah text menjadi lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Menghapus white space\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFrhW9w1c84A"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yYTkzMdBnE"
      },
      "source": [
        "Proses pengurangan berbagai bentuk kata yang berubah menjadi satu bentuk untuk memudahkan analisis, e.g kata dari \"swim\", \"swimming\", \"swims\", \"swam\", adalah semua betuk dari \"swim\". Nah jadi lemma dari kata-kata tersebut adalah \"swim\".\n",
        "\n",
        "Untuk data teks berbahasa Indonesia, kita akan menggunakan library nlp-id. Pertama kita harus menginstallnya terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7FXXPSzdGIx",
        "outputId": "63ef9e1d-20ad-4148-8b09-80b137782a53"
      },
      "source": [
        "!pip install nlp-id"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlp-id\n",
            "  Downloading nlp_id-0.1.12.0.tar.gz (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22\n",
            "  Downloading scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 62.4 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->nlp-id) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.19.5)\n",
            "Building wheels for collected packages: nlp-id, nltk, wget\n",
            "  Building wheel for nlp-id (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlp-id: filename=nlp_id-0.1.12.0-py3-none-any.whl size=8074105 sha256=2d4033248fafed297491658173306938b899d77f0f8448b2041cd47f8f6eafd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/50/48/da59531125bd94f48dfe66140f41d8fd8a4f04062050375013\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=d971495a6853549ea201d1ffe3baa8ed9a1eb43b70e33cd0c0141370254903f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=9a1ea3ca7a6ee7e2574f460a2e96ee26bac82ab43ea88331369991de87ad47a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built nlp-id nltk wget\n",
            "Installing collected packages: wget, scikit-learn, nltk, nlp-id\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nlp-id-0.1.12.0 nltk-3.4.5 scikit-learn-0.22 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH4pX2RxdMdA"
      },
      "source": [
        "Kemudian kita akan menggunakan fungsi _Lemmatizer()_ untuk melakukan lemmatisasi data teks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "JatPKq1kdfKZ",
        "outputId": "57730ead-bc18-4840-80a1-c61b1647ffc6"
      },
      "source": [
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "lemmatizer = Lemmatizer()\n",
        "print(teks[281])\n",
        "teks[281]=lemmatizer.lemmatize(teks[281])\n",
        "teks[281]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sambutan hangat buat pendukung #AHY yang mau bergabung dengan kami <Smiling Face With Smiling Eyes> <FOLDED HANDS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sambut hangat buat dukung ahy yang mau gabung dengan kami smiling face with smiling eyes folded hands'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5XpaVjdme2"
      },
      "source": [
        "#Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4egq54aHdsPr"
      },
      "source": [
        "Stemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan (affixes) baik yang terdiri dari awalan (prefixes), sisipan (infixes), akhiran (suffixes) dan kombinasi dari awalan dan akhiran (cinfixes) pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar dari kata tersebut yang sesuai dengan struktur morfologi Bahasa Indonesia yang baik dan benar.\n",
        "\n",
        "Untuk data teks berbahasa Indonesia, kita akan menggunakan library _PySastrawi_. Pertama kita harus menginstallnya terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax2qvo_9dpRS",
        "outputId": "b7d68855-0fb0-43dd-8e3c-94e65c5fd4e5"
      },
      "source": [
        "!pip install PySastrawi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 45.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 210 kB 12.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPecAFRed2ni"
      },
      "source": [
        "Kemudian kita akan menggunakan fungsi _StemmerFactory()_ untuk melakukan stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "8jsLxl7nd1y-",
        "outputId": "037e8db3-35d8-49e2-d921-e96808caa03b"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "print(teks[129])\n",
        "\n",
        "teks[129] = stemmer.stem(teks[129])\n",
        "teks[129]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#ahy <Frowning Face>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ahy frowning face'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzCD3zQeBA3"
      },
      "source": [
        "#Slang Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYBNWKoQeDs2"
      },
      "source": [
        "Slang adalah kata-kata yang tidak baku secara bahasa namun sering dipakai oleh pengguna bahasa. Kita perlu melakukan standarisasi untuk slang."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5JgWmZ7eGgI"
      },
      "source": [
        "slang_dictionary = pd.read_csv('https://raw.githubusercontent.com/nikovs/data-science-portfolio/master/topic%20modelling/colloquial-indonesian-lexicon.csv')\n",
        "slang_dict = pd.Series(slang_dictionary['formal'].values, index=slang_dictionary['slang']).to_dict()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-fypeWteIHl"
      },
      "source": [
        "def Slangwords(text):\n",
        "    for word in text.split():\n",
        "        if word in slang_dict.keys():\n",
        "            text = text.replace(word, slang_dict[word])\n",
        "    return text"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "4l4K0ujOeLXh",
        "outputId": "ed922642-91e4-4706-f91f-c10084a8866c"
      },
      "source": [
        "print(teks[86])\n",
        "\n",
        "teks[86]=Slangwords(teks[86])\n",
        "teks[86]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lini pertahanan AHY rapuh debat efek, aksi 112 efek dan Antasari efek #pilkadaDKI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lini pertahanan AHY rapuh debat efek, aksi 112 efek dan Antasari efek #pilkadaDKI'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdc1KmH1eNmN"
      },
      "source": [
        "#Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1JBMjoeR3a"
      },
      "source": [
        "Stop words adalah kata umum (common words) yang biasanya mncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval, termasuk oleh Google (penjelasannya di sini). Contoh stop words untuk bahasa Inggis diantaranya, \"of\", \"the\". Sedangkan untuk bahasa Indonesia diantaranya \"yang\", \"di\", \"ke\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYDwCUUeTvp"
      },
      "source": [
        "from nlp_id.stopword import StopWord\n",
        "stopword = StopWord()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "hgpES8h7eXNx",
        "outputId": "7e2f375e-f470-48f9-e0c5-3222c5121d7f"
      },
      "source": [
        "from nlp_id.stopword import StopWord\n",
        "print(teks[99])\n",
        "\n",
        "teks[99]=stopword.remove_stopword(teks[99])\n",
        "teks[99]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heeebohhhh....Agus Sylvi kalah di TPS 6 tmpat @AgusYudhoyono menggunakan hak pilihnya... #PilkadaDKI #QuickCount #ahy\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'heeebohhhh.... Agus Sylvi kalah TPS 6 tmpat@AgusYudhoyono hak pilihnya...#PilkadaDKI#QuickCount#ahy'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzSUvL1KeZa4"
      },
      "source": [
        "#Unwanted Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY8AMxXhecPm"
      },
      "source": [
        "Unwanted words adalah kata-kata yang berada di luar beberpa hal di atas namun perlu kita hapus. Kita bisa mendefinisikan sendiri kata-kata atau karakter yang ingin kita hilangkan dari data teks yang kita peroleh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUMs9KWaed1R"
      },
      "source": [
        "unwanted_words = ['sy', 'karna', 'gue', 'pun', 'nya', 'yg', 'gw', 'ke', 'gak', \n",
        "                 'ga', 'buat', 'selama', 'akan', 'gua', 'gw', 'gue', 'banget', \n",
        "                 'mohon', 'dii', 'kalo', 'dll', 'cuman', 'cuma', 'biar', 'kayak', \n",
        "                 'ssaja', 'sih', 'si', 'situ', 'e', 'diin', 'serba', 'untuj', 'deh', \n",
        "                 'jd', 'ku', 'lg', 'and', 'tuh', 'nih', 'mas', 'mbak', 'tau', 'iya',\n",
        "                 'ya', 'lu', 'pas', 'wkwk', 'haha', 'wkwkwk', 'wkwkw', 'wow', 'akak',\n",
        "                 'anjir', 'lo', 'loh', 'bang', 'kak', 'jd', 'eh', 'oh', 'yuk', 'gila',\n",
        "                 'asa', 'mending', 'engenggak', 'a', 'mah', 'kali', 'piss', 'dlu', 'eh', 'ttp']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33u3erHHegux",
        "outputId": "414bd25f-f679-4e09-bea9-227bbd0af228"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def RemoveUnwantedWords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence =[word for word in word_tokens if not word in unwanted_words]\n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "vINOtil0ejjF",
        "outputId": "b3c8a5bd-435c-453a-fa19-eaea99f08391"
      },
      "source": [
        "print(teks[241])\n",
        "\n",
        "teks[241] = RemoveUnwantedWords(teks[241])\n",
        "teks[241]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yuk #AHY Fans dukung dan menangkan #GubernurMuslim\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Yuk # AHY Fans dukung dan menangkan # GubernurMuslim'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPSJbjnhemMg"
      },
      "source": [
        "#Menerapkan Semua Langkah"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_0mhhmeeqXW",
        "outputId": "7a22282c-e6a7-4208-b75d-651ed41f8bf0"
      },
      "source": [
        "Hasil_Scrapping['content_processed'] = ''\n",
        "\n",
        "for i, row in Hasil_Scrapping.iterrows():\n",
        "    text_tweet = Hasil_Scrapping['Text Tweet'][i]\n",
        "    result = Case_Folding(text_tweet)\n",
        "    result = lemmatizer.lemmatize(result)\n",
        "    result = stemmer.stem(result)\n",
        "    result = stopword.remove_stopword(result)\n",
        "    result = RemoveUnwantedWords(result)\n",
        "    Hasil_Scrapping['content_processed'][i]=result"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "V1bLwsc_esRo",
        "outputId": "5ef38d46-7221-4c5c-fd43-9ceb4fbf8927"
      },
      "source": [
        "Hasil_Scrapping[['Text Tweet', 'content_processed']]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text Tweet</th>\n",
              "      <th>content_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
              "      <td>akun kloning dukung agussilvy serang paslon an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
              "      <td>agussilvy bicara kasihan yaa lap air mata wkwk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
              "      <td>nunggu hasil qc tp nunggu motif cuit sbyudhoyo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
              "      <td>kasi kasi peluru rw agussilvy mempan menangin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
              "      <td>maaf dukung agussilvy hayo dukung aniessandi p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>Kali saja bpk @aniesbaswedan @sandiuno lihat, ...</td>\n",
              "      <td>bpk aniesbaswedan sandiuno lihat rspun selfie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>Kita harus dapat merangkul semua orang tanpa b...</td>\n",
              "      <td>rangkul orang batas usia kelamin okeoce ok han...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>Ini jagoanku dibidang digital &lt;Smiling Face Wi...</td>\n",
              "      <td>jago bidang digital smiling face with sunglass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>#PesanBijak #OkeOce #GubernurGu3 ...</td>\n",
              "      <td>pesanbijak okeoce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...</td>\n",
              "      <td>sandiaga bangun rumah dp simpel banding tol ci...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Text Tweet                                  content_processed\n",
              "0    Banyak akun kloning seolah2 pendukung #agussil...  akun kloning dukung agussilvy serang paslon an...\n",
              "1    #agussilvy bicara apa kasihan yaa...lap itu ai...  agussilvy bicara kasihan yaa lap air mata wkwk...\n",
              "2    Kalau aku sih gak nunggu hasil akhir QC tp lag...  nunggu hasil qc tp nunggu motif cuit sbyudhoyo...\n",
              "3    Kasian oh kasian dengan peluru 1milyar untuk t...  kasi kasi peluru rw agussilvy mempan menangin ...\n",
              "4    Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...  maaf dukung agussilvy hayo dukung aniessandi p...\n",
              "..                                                 ...                                                ...\n",
              "895  Kali saja bpk @aniesbaswedan @sandiuno lihat, ...  bpk aniesbaswedan sandiuno lihat rspun selfie ...\n",
              "896  Kita harus dapat merangkul semua orang tanpa b...  rangkul orang batas usia kelamin okeoce ok han...\n",
              "897  Ini jagoanku dibidang digital <Smiling Face Wi...  jago bidang digital smiling face with sunglass...\n",
              "898               #PesanBijak #OkeOce #GubernurGu3 ...                                  pesanbijak okeoce\n",
              "899  Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...  sandiaga bangun rumah dp simpel banding tol ci...\n",
              "\n",
              "[900 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGwl0qbex9z"
      },
      "source": [
        "Hasil_Scrapping.to_csv('Pilkada_DKI.csv', index=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z2qjppbe2Fn",
        "outputId": "47316d77-1e52-4faa-b469-179ba936bb17"
      },
      "source": [
        "Hasil_Scrapping.info()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 900 entries, 0 to 899\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Id                 900 non-null    int64 \n",
            " 1   Sentiment          900 non-null    object\n",
            " 2   Pasangan Calon     900 non-null    object\n",
            " 3   Text Tweet         900 non-null    object\n",
            " 4   content_processed  900 non-null    object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 35.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vp0XYdepxX"
      },
      "source": [
        ""
      ]
    }
  ]
}